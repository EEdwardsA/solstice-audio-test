<html>
<h1>Audio Demo</h1>

Demonstration of using AudioWorklets to read from the mic and do stuff with it.
As of 2020-05 this depends on an API that is implemented in
Firefox 76 and the Blink-based browsers: Chrome, Edge, Brave, and
Opera.

<p>

Headphones are recommended, to avoid feedback.

<p>
  Input device:
  <select id=inSelect disabled=true>
    <option>Loading...</option>
  </select>
  <!-- It turns out that output device selection is a hacky mess, so for now we'll skip it.
    See: https://stackoverflow.com/questions/41863094/how-to-select-destination-output-device-using-web-audio-api -->
<p>
  <button id=startButton disabled=true>Start</button>
  <button id=stopButton disabled=true>Stop</button>
<p>

<i><span id=recording></span></i>

<script>
const FRAME_SIZE = 128;  // by Web Audio API spec
const SAMPLE_BATCH_SIZE = 10;

async function force_permission_prompt() {
  // In order to enumerate devices, we must first force a permission prompt by opening a device and then closing it again.
  // See: https://stackoverflow.com/questions/60297972/navigator-mediadevices-enumeratedevices-returns-empty-labels
  stream = await navigator.mediaDevices.getUserMedia({audio: true});
  stream.getTracks().forEach((track) => track.stop());
}

async function wait_for_mic_permissions() {
  var perm_status = await navigator.permissions.query({name: "microphone"});
  if (perm_status.state == "granted" || perm_status.state == "denied") {
    return;
  } else {
    force_permission_prompt();
    return new Promise((resolve, reject) => {
      perm_status.onchange = (e) => {
        if (e.target.state == "granted" || e.target.state == "denied") {
          resolve();
        }
      }
    });
  }
}

async function enumerate_devices() {
  navigator.mediaDevices.enumerateDevices().then((devices) => {
    var in_list = document.getElementById('inSelect');

    // Clear existing entries
    in_list.options.length = 0;

    devices.forEach((info) => {
      var el = document.createElement("option");
      el.value = info.deviceId;
      if (info.kind === 'audioinput') {
        el.text = info.label || 'Unknown Microphone';
        in_list.appendChild(el);
      }
    });
  });
}

var audioCtx;

var start_button = document.getElementById('startButton');
var stop_button = document.getElementById('stopButton');
var in_select = document.getElementById('inSelect');

async function initialize() {
  await wait_for_mic_permissions();
  await enumerate_devices();
  in_select.disabled = false;
  start_button.disabled = false;
}

async function start() {
  start_button.disabled = true;
  stop_button.disabled = false;
  in_select.disabled = true;

  var micStream = await navigator.mediaDevices.getUserMedia({
    audio: {
      echoCancellation: false,
      noiseSuppression: false,
      autoGainControl: false,
      deviceId: { exact: in_select.options[in_select.selectedIndex].value },
    },
  });

  audioCtx = new (window.AudioContext || window.webkitAudioContext)({});

  var micNode = new MediaStreamAudioSourceNode(audioCtx, { mediaStream: micStream });

  var now = Date.now();  // cachebuster
  await audioCtx.audioWorklet.addModule('audio-worklet-in-to-out.js?' + now);
  var playerNode = new AudioWorkletNode(audioCtx, 'player');
  var ctr = 0;

  var outbuf = new Float32Array(FRAME_SIZE * SAMPLE_BATCH_SIZE);
  var outbuf_wrptr = 0;

  playerNode.port.onmessage = (event) => {
    ctr++;
    if (ctr % 100 == 0) {
      //console.log("In main thread onmessage: received ", ctr, event.data);
    }

    outbuf.set(event.data, outbuf_wrptr);
    outbuf_wrptr += event.data.length;

    if (outbuf_wrptr == outbuf.length) {
      var xhr = new XMLHttpRequest();
      xhr.onreadystatechange = () => {
        if (xhr.readyState == 4 && xhr.status == 200) {
          //console.log(xhr.responseText);
          result = new Float32Array(xhr.response);
          playerNode.port.postMessage(result, [result.buffer]);
        }
      }
      var layer = window.location.hash.substr(1) || 1;
      xhr.open("POST", "http://localhost:8081/" + layer, true);
      xhr.setRequestHeader('Content-Type', 'application/octet-stream');
      xhr.responseType = "arraybuffer";
      xhr.send(outbuf);

      // TODO: I'm not sure if we could reuse the buffer, or if we could race with the outgoing XHR.
      outbuf = new Float32Array(FRAME_SIZE * SAMPLE_BATCH_SIZE);
      outbuf_wrptr = 0;
    }
  };
  micNode.connect(playerNode);
  playerNode.connect(audioCtx.destination);
}

async function stop() {
  await audioCtx.close();

  start_button.disabled = false;
  stop_button.disabled = true;
  in_select.disabled = false;
}

document.getElementById('startButton').addEventListener("click", start);
document.getElementById('stopButton').addEventListener("click", stop);

initialize();
</script>
</html>
